---
title: "Modeling classes"
author: "jvicentem"
date: "19 de diciembre de 2017"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)

library(caret)

library(doMC)
registerDoMC(cores = detectCores(all.tests = FALSE, logical = TRUE))

library(parallel)

library(rattle)

library(rpart.plot)

library(dummies)

library(mice)

library(NbClust)

library(fpc) 
#devtools::install_github("ujjwalkarn/xda")
```

```{r train-with-quality-as-factors}
train_set_classes <- train_set %>%
                      mutate(quality = as.factor(quality))

whole_data_classes <- complete(pmm_imp, action = 2) %>% 
                        mutate(quality = as.factor(quality)) %>% 
                        select(quality)
```


```{r classes-eda}
as.data.frame(table(Quality = whole_data_classes$quality))
as.data.frame(prop.table(table(Quality = whole_data_classes$quality)))

histogram(whole_data_classes$quality)

xda::charSummary(whole_data_classes %>% select(quality))
```

```{r classes-eda-only-train}
as.data.frame(table(Quality = train_set_classes$quality))
as.data.frame(prop.table(table(Quality = train_set_classes$quality)))

histogram(train_set_classes$quality)

xda::charSummary(train_set_classes %>% select(quality))
```

In order to make all classes have the same number of observations I would bind the classes 3, 4 and 5 together (class C), then class 6 (class B) and finally create a new class that contains classes 7, 8 and 9 (class A). So now the problem is a multiclass problem with 3 classes. After that I think applying an under-sampling method would make the number of observations in each class the same.

However, I have some questions about this: Would that make sense from a wine expert point of view? Are wines rated 3 or 4 as bad as wines rated 5? Are wines rated 7 as good as wines rated 8 or 9? 

These questions show me the importance of expertise in the domain you are working on and in the domain of the problem you're trying to solve. Since the purpose of this project is to learn and train my data science skills, I'll go on assuming the binding of classes I did makes sense.

```{r solving-unbalanced-classes}
train_set_classes_balanced <- train_set_classes

train_set_classes_balanced$quality <- as.numeric(as.character(train_set_classes_balanced$quality))

train_set_classes_balanced <- train_set_classes_balanced %>% 
                                mutate(b_qual = ifelse(quality >= 3 & quality <= 5, 'C', ifelse(quality == 6, 'B', 'A')))

train_set_classes_balanced$b_qual <- as.factor(train_set_classes_balanced$b_qual)
                                
# Showing frequencies and histogram
as.data.frame(table(Quality = train_set_classes_balanced$b_qual))
histogram(train_set_classes_balanced$b_qual)

# Performing under-sampling
minority_class_obs <- nrow(train_set_classes_balanced %>% filter(b_qual == 'A'))

random_class_B_indexes <- sample(which(train_set_classes_balanced$b_qual == 'B'), minority_class_obs, replace=TRUE)

random_class_C_indexes <- sample(which(train_set_classes_balanced$b_qual == 'C'), minority_class_obs, replace=TRUE)

train_set_classes_balanced <- rbind(train_set_classes_balanced[random_class_B_indexes,] , 
                                    rbind(train_set_classes_balanced[random_class_C_indexes,] ,
                                          train_set_classes_balanced %>% filter(b_qual == 'A')
                                          )  
                                    )

# Showing frequencies and histogram again
as.data.frame(table(Quality = train_set_classes_balanced$b_qual))
histogram(train_set_classes_balanced$b_qual)
```

The three classes are balanced now. Please have in mind we've lost 2133 observations and it's very likely we've lost valuable observations. Maybe we should generate a set of balanced datasets and stick with the one which shows better performance in the models. There are other unbalancing techniques which aim to balance classes in a smart way. I used some of these techniques in this project https://github.com/jvicentem/phishing-website-detection. 

For now, we're gonna stick with the simple under-sampling method in this project.


I've just realised the bining I did might be awful: a wine could be correctly classified as class A or C but we don't know its exact quality class, so maybe we are classifying wines with an error up to 2 units, and remember the regression models we used previously, thogh it might not be the right approach, the errors in dev set were between 0.6 and 0.8.

Before going on to training the models I'd like to use k-means in order to see how this algorithm clusters the observations and hopefully cand help me to find a better way to group the observations.

```{r k-means-clustering-1}
# Let's use NbClust to determine the optimal number of clusters
seed <- 2060786142
set.seed(seed)

normalize <- function(x) (x-min(x))/(max(x) - min(x))

nbc_results <- NbClust(data = whole_data %>% select(-wine_colour) %>% mutate_all(funs(normalize)), distance = "euclidean", min.nc = 2, max.nc = 10, method = "kmeans", index = "all")
```

```{r k-means-clustering-2}
# Go on coding in this chunk becase I don't want to lose the NbClust output (it takes some time to finish)

kmeans_result <- kmeans(whole_data %>% select(-wine_colour) %>% mutate_all(funs(normalize)), 3, algorithm = "Hartigan-Wong", trace=FALSE)

plotcluster(whole_data %>% select(-wine_colour) %>% mutate_all(funs(normalize)), kmeans_result$cluster)

```

Having in mind we're using discriminant components to visualize the clusters in 2D, it doesn't look bad. I'm gonna explore the data which belongs to each of the clusters to see if I can still predict wine quality by using classification models. My gut says I'm gonna have the same problem as above: by binding many quality classes my prediction will be less accurate than using the regression approach. But we'll see... In that case I'd try to predict the wine colour.


Oh, the following chunks belong to the regression part I did previously. I copied and pasted since most of the code I'm gonna use in this classification problem will be the same so do me a favour and ignore them for a while until I have some free time to work on the models :)

```{r cart}
all_indexes <- 1:nrow(train_set)

mean_median_errors <- function(folds_errors) {
  square <- function(x) x^2
  
  mses <- foreach(x=folds_errors$RMSE, .combine='cbind') %do% square(x)
  
  print('MSE:')
  print(paste('Mean:', mean(mses)))
  print(paste('Median:', median(mses)))
  print('')
  print('RMSE:')
  print(paste('Mean:', mean(folds_errors$RMSE)))
  print(paste('Median:', median(folds_errors$RMSE)))
  print('')
  print('MAE:')
  print(paste('Mean:', mean(folds_errors$MAE)))
  print(paste('Median:', median(folds_errors$MAE)))  
}

rpart_with_folds_cl <- function(train_df) {
  # Best cp: 0.005
  # tr <- train(y = train_df$quality,
  #       x = train_df %>% select(-quality),
  #       method='rpart',
  #       trControl=trainControl(method = 'cv', 
  #                    allowParallel = TRUE,
  #                    index=list(
  #                      f_1 = all_indexes[-folds_indexes$Fold1],
  #                      f_2 = all_indexes[-folds_indexes$Fold2],
  #                      f_3 = all_indexes[-folds_indexes$Fold3],
  #                      f_4 = all_indexes[-folds_indexes$Fold4],
  #                      f_5 = all_indexes[-folds_indexes$Fold5]
  #                    ),
  #                    indexOut=list(
  #                      f_1 = folds_indexes$Fold1,
  #                      f_2 = folds_indexes$Fold2,
  #                      f_3 = folds_indexes$Fold3,
  #                      f_4 = folds_indexes$Fold4,
  #                      f_5 = folds_indexes$Fold5
  #                    ),
  #                    verboseIter = TRUE,
  #                    savePredictions = TRUE
  #                  ),
  #       tuneGrid=expand.grid(cp=seq(0, 0.1, 0.005)),
  #       metric='RMSE',
  #       maximize=FALSE,
  #       preProcess=c('center','scale'))
  
  # Best depth: 8
  # tr <- train(y = train_df$quality,
  #       x = train_df %>% select(-quality),
  #       method='rpart2',
  #       trControl=trainControl(method = 'cv', 
  #                    allowParallel = TRUE,
  #                    index=list(
  #                      f_1 = all_indexes[-folds_indexes$Fold1],
  #                      f_2 = all_indexes[-folds_indexes$Fold2],
  #                      f_3 = all_indexes[-folds_indexes$Fold3],
  #                      f_4 = all_indexes[-folds_indexes$Fold4],
  #                      f_5 = all_indexes[-folds_indexes$Fold5]
  #                    ),
  #                    indexOut=list(
  #                      f_1 = folds_indexes$Fold1,
  #                      f_2 = folds_indexes$Fold2,
  #                      f_3 = folds_indexes$Fold3,
  #                      f_4 = folds_indexes$Fold4,
  #                      f_5 = folds_indexes$Fold5
  #                    ),
  #                    verboseIter = TRUE,
  #                    savePredictions = TRUE
  #                  ),
  #       tuneGrid=expand.grid(maxdepth=c(4,6,8,9,10,12,14,15,19,25)),
  #       metric='RMSE',
  #       maximize=FALSE,
  #       preProcess=c('center','scale'))
  
  # With max depth = 8, best cp is 0.005
  # tr <- train(y = train_df$quality,
  #       x = train_df %>% select(-quality),
  #       method='rpart',
  #       trControl=trainControl(method = 'cv', 
  #                    allowParallel = TRUE,
  #                    index=list(
  #                      f_1 = all_indexes[-folds_indexes$Fold1],
  #                      f_2 = all_indexes[-folds_indexes$Fold2],
  #                      f_3 = all_indexes[-folds_indexes$Fold3],
  #                      f_4 = all_indexes[-folds_indexes$Fold4],
  #                      f_5 = all_indexes[-folds_indexes$Fold5]
  #                    ),
  #                    indexOut=list(
  #                      f_1 = folds_indexes$Fold1,
  #                      f_2 = folds_indexes$Fold2,
  #                      f_3 = folds_indexes$Fold3,
  #                      f_4 = folds_indexes$Fold4,
  #                      f_5 = folds_indexes$Fold5
  #                    ),
  #                    verboseIter = TRUE,
  #                    savePredictions = TRUE
  #                  ),
  #       tuneGrid=expand.grid(cp=seq(0, 0.1, 0.005)),
  #       metric='RMSE',
  #       maximize=FALSE,
  #       control = rpart.control(maxdepth=8),
  #       preProcess=c('center','scale'))
  
  # Tweaking minsplit. By default it was 20. Best minsplit: ~100 
  #minbucket is by default minsplit/3  which I think is pretty fair.
  # tr <- train(y = train_df$quality,
  #       x = train_df %>% select(-quality),
  #       method='rpart',
  #       trControl=trainControl(method = 'cv', 
  #                    allowParallel = TRUE,
  #                    index=list(
  #                      f_1 = all_indexes[-folds_indexes$Fold1],
  #                      f_2 = all_indexes[-folds_indexes$Fold2],
  #                      f_3 = all_indexes[-folds_indexes$Fold3],
  #                      f_4 = all_indexes[-folds_indexes$Fold4],
  #                      f_5 = all_indexes[-folds_indexes$Fold5]
  #                    ),
  #                    indexOut=list(
  #                      f_1 = folds_indexes$Fold1,
  #                      f_2 = folds_indexes$Fold2,
  #                      f_3 = folds_indexes$Fold3,
  #                      f_4 = folds_indexes$Fold4,
  #                      f_5 = folds_indexes$Fold5
  #                    ),
  #                    verboseIter = TRUE,
  #                    savePredictions = TRUE
  #                  ),
  #       tuneGrid=expand.grid(cp=0.005),
  #       metric='RMSE',
  #       maximize=FALSE,
  #       control = rpart.control(maxdepth=8, minsplit=0.025*nrow(train_df)),
  #       preProcess=c('center','scale'))
  
  # Tweaking split criterion: information or gini (default). No difference between them
  tr <- train(y = train_df$quality,
              x = train_df %>% select(-quality),
              method='rpart',
              trControl=trainControl(method = 'cv', 
                          allowParallel = TRUE,
                          index=list(
                            f_1 = all_indexes[-folds_indexes$Fold1],
                            f_2 = all_indexes[-folds_indexes$Fold2],
                            f_3 = all_indexes[-folds_indexes$Fold3],
                            f_4 = all_indexes[-folds_indexes$Fold4],
                            f_5 = all_indexes[-folds_indexes$Fold5]
                          ),
                          indexOut=list(
                            f_1 = folds_indexes$Fold1,
                            f_2 = folds_indexes$Fold2,
                            f_3 = folds_indexes$Fold3,
                            f_4 = folds_indexes$Fold4,
                            f_5 = folds_indexes$Fold5
                          ),
                          verboseIter = TRUE,
                          savePredictions = TRUE
                        ),
              preProcess=c('center', 'scale'),
              tuneGrid=expand.grid(cp=0.005),
              metric='RMSE',
              maximize=FALSE,
              control = rpart.control(maxdepth=8, minsplit=0.025*nrow(train_df), parms=list(split='information')),
              preProcess=c('center','scale'))
  
  print(tr$resample)
  print('Best tune')
  print(tr$bestTune)
  
  return(tr)
}

error_metrics_cart <- cart_with_folds(train_set)$resample

mean_median_errors(error_metrics_cart)

# Now we train a cart with the whole train set using the best set of parameters tested in the different folds
cart <- train(y = train_set$quality,
              x = train_set %>% select(-quality),
              method='rpart',
              trControl=trainControl(method = 'none', allowParallel = TRUE),
              tuneGrid=expand.grid(cp=0.005),
              metric='RMSE',
              maximize=FALSE,
              control = rpart.control(maxdepth=8, minsplit=0.025*nrow(train_set)),
              preProcess=c('center','scale'))
  
fancyRpartPlot(cart$finalModel)
prp(cart$finalModel) 

plot(varImp(cart))
```

```{r rf}
rf_train_set <- train_set
names(rf_train_set) <- make.names(names(rf_train_set), unique=TRUE)
rf_dev_test <- dev_test
names(rf_dev_test) <- make.names(names(rf_dev_test), unique=TRUE)
  
rf_with_folds <- function(train_df) {
  # 8 is the best value for mtry
  # tr <- train(y=train_df$quality,
  #            x=train_df %>% select(-quality),
  #            method='ranger',
  #            trControl=trainControl(method = 'cv', 
  #                    allowParallel = TRUE,
  #                    index=list(
  #                      f_1 = all_indexes[-folds_indexes$Fold1],
  #                      f_2 = all_indexes[-folds_indexes$Fold2],
  #                      f_3 = all_indexes[-folds_indexes$Fold3],
  #                      f_4 = all_indexes[-folds_indexes$Fold4],
  #                      f_5 = all_indexes[-folds_indexes$Fold5]
  #                    ),
  #                    indexOut=list(
  #                      f_1 = folds_indexes$Fold1,
  #                      f_2 = folds_indexes$Fold2,
  #                      f_3 = folds_indexes$Fold3,
  #                      f_4 = folds_indexes$Fold4,
  #                      f_5 = folds_indexes$Fold5
  #                    ),
  #                    verboseIter = TRUE,
  #                    savePredictions = TRUE
  #                  ),
  #            preProc=c('center', 'scale'),
  #            tuneGrid=expand.grid(mtry = c(4,6,8,9,10), splitrule='variance'),
  #            num.trees=200,
  #            importance='impurity')
  
  # Trying to remove from the model some of the less important variables in
  # cart  
  tr <- train(y=train_df$quality,
             x=train_df %>% select(-quality, -pH, -other.sulfur.dioxide),
             method='ranger',
             trControl=trainControl(method = 'cv', allowParallel = TRUE,
               index=list(
                 f_1 = all_indexes[-folds_indexes$Fold1],
                 f_2 = all_indexes[-folds_indexes$Fold2],
                 f_3 = all_indexes[-folds_indexes$Fold3],
                 f_4 = all_indexes[-folds_indexes$Fold4],
                 f_5 = all_indexes[-folds_indexes$Fold5]
               ),
               indexOut=list(
                 f_1 = folds_indexes$Fold1,
                 f_2 = folds_indexes$Fold2,
                 f_3 = folds_indexes$Fold3,
                 f_4 = folds_indexes$Fold4,
                 f_5 = folds_indexes$Fold5
               ),                
               verboseIter = TRUE,
               savePredictions = TRUE
             ),
             preProc=c('center', 'scale'),
             tuneGrid=expand.grid(mtry = c(4,6,8,9), splitrule='variance'),
             num.trees=200,
             always.split.variables=c('volatile.acidity', 'alcohol'),
             importance='impurity')
  
  print(tr$resample)
  print('Best tune')
  print(tr$bestTune)
  
  return(tr)
}

error_metrics_rf <- rf_with_folds(rf_train_set)$resample

mean_median_errors(error_metrics_rf)

# Now we train a random forest model with the whole train set using the best set of parameters tested in the different folds
rf <- train(y = rf_train_set$quality,
            x = rf_train_set %>% select(-quality, -pH, -other.sulfur.dioxide),
            method='ranger',
            trControl=trainControl(method = 'none', allowParallel = TRUE),
            preProc=c('center', 'scale'),
            tuneGrid=expand.grid(mtry = 4, splitrule='variance'),
            num.trees=200,
            always.split.variables=c('volatile.acidity', 'alcohol'),
            importance='impurity')

plot(varImp(rf))
```


```{r lsvm}
svm_train_set <- train_set %>% mutate(wine_colour = as.factor(wine_colour))
svm_train_set <- cbind(svm_train_set %>% select(-wine_colour), dummy(svm_train_set$wine_colour, sep="."))
colnames(svm_train_set) <- c(colnames(svm_train_set)[1:12], c('wine_colour.red', 'wine_colour.white'))
svm_train_set <- svm_train_set %>% select(-wine_colour.red) # White wine is the majority level so that will be the base level in this dummy variable

svm_dev_test <- dev_test %>% mutate(wine_colour = as.factor(wine_colour))
svm_dev_test <- cbind(svm_dev_test %>% select(-wine_colour), dummy(svm_dev_test$wine_colour, sep="."))
colnames(svm_dev_test) <- c(colnames(svm_dev_test)[1:12], c('wine_colour.red', 'wine_colour.white'))
svm_dev_test <- svm_dev_test %>% select(-wine_colour.red) 

lsvm_with_folds <- function(train_df) {
  # tr <- train(y=train_df$quality,
  #             x=train_df %>% select (-quality),
  #             method='svmLinear2',
  #             trControl=trainControl(method = 'cv', allowParallel = TRUE,
  #              index=list(
  #                f_1 = all_indexes[-folds_indexes$Fold1],
  #                f_2 = all_indexes[-folds_indexes$Fold2],
  #                f_3 = all_indexes[-folds_indexes$Fold3],
  #                f_4 = all_indexes[-folds_indexes$Fold4],
  #                f_5 = all_indexes[-folds_indexes$Fold5]
  #              ),
  #              indexOut=list(
  #                f_1 = folds_indexes$Fold1,
  #                f_2 = folds_indexes$Fold2,
  #                f_3 = folds_indexes$Fold3,
  #                f_4 = folds_indexes$Fold4,
  #                f_5 = folds_indexes$Fold5
  #              ),
  #              verboseIter = TRUE,
  #              savePredictions = TRUE
  #             ),
  #             preProc=c('center', 'scale'),
  #             tuneGrid=expand.grid(cost=c(0, 0.01, 0.1, 0.2, 1))
  #          )
  
  tr <- train(y=train_df$quality,
              x=train_df %>% select (-quality, -pH),
              method='svmLinear2',
              trControl=trainControl(method = 'cv', allowParallel = TRUE,
               index=list(
                 f_1 = all_indexes[-folds_indexes$Fold1],
                 f_2 = all_indexes[-folds_indexes$Fold2],
                 f_3 = all_indexes[-folds_indexes$Fold3],
                 f_4 = all_indexes[-folds_indexes$Fold4],
                 f_5 = all_indexes[-folds_indexes$Fold5]
               ),
               indexOut=list(
                 f_1 = folds_indexes$Fold1,
                 f_2 = folds_indexes$Fold2,
                 f_3 = folds_indexes$Fold3,
                 f_4 = folds_indexes$Fold4,
                 f_5 = folds_indexes$Fold5
               ),
               verboseIter = TRUE,
               savePredictions = TRUE
              ),
              preProc=c('center', 'scale'),
              tuneGrid=expand.grid(cost=c(0, 0.01, 0.015, 0.02, 0.1, 0.2, 1))
           )  
  
  print(tr$resample)
  print('Best tune')
  print(tr$bestTune)
  
  return(tr)
}

lsvm_train <- lsvm_with_folds(svm_train_set)

error_metrics_lsvm <- lsvm_train$resample
mean_median_errors(error_metrics_lsvm)

lsvm <- train(y=svm_train_set$quality,
              x=svm_train_set %>% select (-quality, -pH),
              method='svmLinear2',
              trControl=trainControl(method = 'none', allowParallel = TRUE),
              preProc=c('center', 'scale'),
              tuneGrid=expand.grid(cost=c(0.01))
             )

varImp(lsvm)
plot(varImp(lsvm))
```

```{r psvm}
# psvm_with_folds <- function(train_df) {
#   tr <- train(y = train_df$quality,
#               x = train_df %>% select(-quality),
#               method='svmPoly',
#               trControl=trainControl(method = 'cv', allowParallel = TRUE,
#                 index=list(
#                   f_1 = all_indexes[-folds_indexes$Fold1],
#                   f_2 = all_indexes[-folds_indexes$Fold2],
#                   f_3 = all_indexes[-folds_indexes$Fold3],
#                   f_4 = all_indexes[-folds_indexes$Fold4],
#                   f_5 = all_indexes[-folds_indexes$Fold5]
#                 ),
#                 indexOut=list(
#                   f_1 = folds_indexes$Fold1,
#                   f_2 = folds_indexes$Fold2,
#                   f_3 = folds_indexes$Fold3,
#                   f_4 = folds_indexes$Fold4,
#                   f_5 = folds_indexes$Fold5
#                 ),
#                 verboseIter = TRUE,
#                 savePredictions = TRUE
#               ),
#               preProcess=c('center', 'scale'),
#               tuneGrid=expand.grid(C=c(0, 0.01, 0.1, 0.015, 0.02, 1), degree=c(2, 4, 6), scale=c(1))
#         )  
#   
#   print(tr$resample)
#   print('Best tune')
#   print(tr$bestTune)
#   
#   return(tr)
# }
# 
# psvm_train <- psvm_with_folds(svm_train_set[1:5,])
# error_metrics_psvm <- psvm_train$resample
# 
# mean_median_errors(error_metrics_psvm)
# 
# psvm <- train(y=svm_train_set$quality,
#               x=svm_train_set %>% select (-quality, -pH),
#               method='svmPoly',
#               trControl=trainControl(method = 'none', allowParallel = TRUE),
#               preProc=c('center', 'scale'),
#               tuneGrid=expand.grid(C=c(0.01), degree=c(2), scale=c(1))
#              )
# 
# varImp(psvm)
# plot(varImp(psvm))
```

```{r xgboost}
xgb_with_folds <- function(train_df) {
  tr <- train(y=train_df$quality,
             x=train_df %>% select(-quality, -wine_colour.white),
             method='xgbLinear',
             trControl=trainControl(method = 'cv', allowParallel = TRUE,
               index=list(
                 f_1 = all_indexes[-folds_indexes$Fold1],
                 f_2 = all_indexes[-folds_indexes$Fold2],
                 f_3 = all_indexes[-folds_indexes$Fold3],
                 f_4 = all_indexes[-folds_indexes$Fold4],
                 f_5 = all_indexes[-folds_indexes$Fold5]
               ),
               indexOut=list(
                 f_1 = folds_indexes$Fold1,
                 f_2 = folds_indexes$Fold2,
                 f_3 = folds_indexes$Fold3,
                 f_4 = folds_indexes$Fold4,
                 f_5 = folds_indexes$Fold5
               ),                
               verboseIter = TRUE,
               savePredictions = TRUE
             ),
             preProc=c('center', 'scale'),
             tuneGrid=expand.grid(nrounds = c(1000), lambda = c(0, 0.5, 1), alpha = c(0, 0.5, 0.8, 1), eta = c(0.005, 0.01, 0.02))
             )
  
  print(tr$resample)
  print('Best tune')
  print(tr$bestTune)
  
  return(tr)
}

xgb_train <- xgb_with_folds(svm_train_set)
error_metrics_xgb <- xgb_train$resample

mean_median_errors(error_metrics_xgb)

xgb <- train(y=svm_train_set$quality,
              x=svm_train_set %>% select (-quality, -wine_colour.white),
              method='xgbLinear',
              trControl=trainControl(method = 'none', allowParallel = TRUE),
              preProc=c('center', 'scale'),
              # Though CV output says eta = 0.005 is better, I'm gonna stick with 0.01 because
              #the error metrics are equal and this last value can make training a little bit faster
              tuneGrid=expand.grid(nrounds = c(10000), lambda = c(0), alpha = c(1), eta = c(0.01))
             )

varImp(xgb)
plot(varImp(xgb))
```
