---
title: "Testing models"
author: "jvicentem"
date: "17 December 2017"
output: html_document
---

```{r setup, include=FALSE}
library(caret)
```

```{r testing-models}
print('------------- CART ---------------')
print('Train results')
postResample(pred = predict(cart, train_set), obs = train_set$quality) 
print('Test results')
postResample(pred = predict(cart, test_set), obs = test_set$quality) 
print('')
print('------------- Random Forest ---------------')
print('Train results')
postResample(pred = predict(rf, rf_train_set), obs = rf_train_set$quality)
print('Test results')
postResample(pred = predict(rf, rf_test_set), obs = rf_test_set$quality)
print('')
print('-------------- Linear Kernel SVM ---------------')
print('Train results')
postResample(pred = predict(lsvm, svm_train_set), obs = svm_train_set$quality)
print('Test results')
postResample(pred = predict(lsvm, svm_test_set), obs = svm_test_set$quality)
#print()
# print('------------ Polynomial Kernel SVM -------------')
# print('Test results')
# postResample(pred = predict(psvm, svm_test_set), obs = svm_test_set$quality)
# It shows an error I don't know how to fix yet :( 
print('')
print('----------- XGBoost -------------')
# Modifications made on svm test set work for xgb model
print('Train results')
postResample(pred = predict(xgb, svm_train_set), obs = svm_train_set$quality)
print('Test results')
postResample(pred = predict(xgb, svm_test_set), obs = svm_test_set$quality)

```

There's a huge gap between test and train metrics for the Random Forest and XGBoost models: they seem to be overfitted. I'm gonna take a look at this again because I don't remember those models clearly showed to be that overfitted... Stay tuned.
On the other hand, they seem to be the best models. 

Linear SVM seems to have a high bias problem.

Since quality values are integers, I'm gonna try now to round the predicted values and check the performance metrics again.

```{r testing-models-round}
print('------------- CART ---------------')
print('Train results')
postResample(pred = round(predict(cart, train_set)), obs = train_set$quality) 
print('Test results')
postResample(pred = round(predict(cart, test_set)), obs = test_set$quality) 
print('')
print('------------- Random Forest ---------------')
print('Train results')
postResample(pred = round(predict(rf, rf_train_set)), obs = rf_train_set$quality)
print('Test results')
postResample(pred = round(predict(rf, rf_test_set)), obs = rf_test_set$quality)
print('')
print('-------------- Linear Kernel SVM ---------------')
print('Train results')
postResample(pred = round(predict(lsvm, svm_train_set)), obs = svm_train_set$quality)
print('Test results')
postResample(pred = round(predict(lsvm, svm_test_set)), obs = svm_test_set$quality)
#print()
# print('------------ Polynomial Kernel SVM -------------')
# print('Test results')
# postResample(pred = predict(psvm, svm_test_set), obs = svm_test_set$quality)
# It shows an error I don't know how to fix yet :( 
print('')
print('----------- XGBoost -------------')
# Modifications made on svm test set work for xgb model
print('Train results')
postResample(pred = round(predict(xgb, svm_train_set)), obs = svm_train_set$quality)
print('Test results')
postResample(pred = round(predict(xgb, svm_test_set)), obs = svm_test_set$quality)

```

The results are similar. I don't know what the baseline error metric values should be since I'm not a wine expert and I haven't read about previous work with this dataset but RMSE and MAE values don't look too bad to me. However, R² values are very very poor.

I'm gonna dig in a little more and see how the models perform depending on the wine color. From now on I'll round the predicted values.

```{r testing-models-colour}
red_wine_indexes_tr <- which(train_set$wine_colour == 'red')
red_wine_indexes_te <- which(test_set$wine_colour == 'red')
print('Red wine:')
print('------------- CART ---------------')
print('Train results')
postResample(pred = round(predict(cart, train_set[red_wine_indexes_tr,])), obs = train_set$quality[red_wine_indexes_tr]) 
print('Test results')
postResample(pred = round(predict(cart, test_set[red_wine_indexes_te,])), obs = test_set$quality[red_wine_indexes_te]) 
print('')
print('------------- Random Forest ---------------')
print('Train results')
postResample(pred = round(predict(rf, rf_train_set[red_wine_indexes_tr,])), obs = rf_train_set$quality[red_wine_indexes_tr])
print('Test results')
postResample(pred = round(predict(rf, rf_test_set[red_wine_indexes_te,])), obs = rf_test_set$quality[red_wine_indexes_te])
print('')
print('-------------- Linear Kernel SVM ---------------')
print('Train results')
postResample(pred = round(predict(lsvm, svm_train_set[red_wine_indexes_tr,])), obs = svm_train_set$quality[red_wine_indexes_tr])
print('Test results')
postResample(pred = round(predict(lsvm, svm_test_set[red_wine_indexes_te,])), obs = svm_test_set$quality[red_wine_indexes_te])
#print()
# print('------------ Polynomial Kernel SVM -------------')
# print('Test results')
# postResample(pred = predict(psvm, svm_test_set), obs = svm_test_set$quality)
# It shows an error I don't know how to fix yet :( 
print('')
print('----------- XGBoost -------------')
# Modifications made on svm test set work for xgb model
print('Train results')
postResample(pred = round(predict(xgb, svm_train_set[red_wine_indexes_tr,])), obs = svm_train_set$quality[red_wine_indexes_tr])
print('Test results')
postResample(pred = round(predict(xgb, svm_test_set[red_wine_indexes_te,])), obs = svm_test_set$quality[red_wine_indexes_te])


print('')
print('')
print('')
print('')
print('')
print('')

white_wine_indexes_tr <- which(train_set$wine_colour == 'white')
white_wine_indexes_te <- which(test_set$wine_colour == 'white')
print('White wine:')
print('------------- CART ---------------')
print('Train results')
postResample(pred = round(predict(cart, train_set[white_wine_indexes_tr,])), obs = train_set$quality[white_wine_indexes_tr]) 
print('Test results')
postResample(pred = round(predict(cart, test_set[white_wine_indexes_te,])), obs = test_set$quality[white_wine_indexes_te]) 
print('')
print('------------- Random Forest ---------------')
print('Train results')
postResample(pred = round(predict(rf, rf_train_set[white_wine_indexes_tr,])), obs = rf_train_set$quality[white_wine_indexes_tr])
print('Test results')
postResample(pred = round(predict(rf, rf_test_set[white_wine_indexes_te,])), obs = rf_test_set$quality[white_wine_indexes_te])
print('')
print('-------------- Linear Kernel SVM ---------------')
print('Train results')
postResample(pred = round(predict(lsvm, svm_train_set[white_wine_indexes_tr,])), obs = svm_train_set$quality[white_wine_indexes_tr])
print('Test results')
postResample(pred = round(predict(lsvm, svm_test_set[white_wine_indexes_te,])), obs = svm_test_set$quality[white_wine_indexes_te])
#print()
# print('------------ Polynomial Kernel SVM -------------')
# print('Test results')
# postResample(pred = predict(psvm, svm_test_set), obs = svm_test_set$quality)
# It shows an error I don't know how to fix yet :( 
print('----------- XGBoost -------------')
# Modifications made on svm test set work for xgb model
print('Train results')
postResample(pred = round(predict(xgb, svm_train_set[white_wine_indexes_tr,])), obs = svm_train_set$quality[white_wine_indexes_tr])
print('Test results')
postResample(pred = round(predict(xgb, svm_test_set[white_wine_indexes_te,])), obs = svm_test_set$quality[white_wine_indexes_te])
```

There are higher error values in the white wine observations than in the red wine observations. Anyway, there's not a big difference between the two types of wine in terms of model error values.

Next steps:
* Apply other transformations to the variables.
* Use the PCA previously done to speed up xgboost and svms training times
* Cluster the observations and apply a model in each cluster (¿maybe one model for white wine and another model for red wine?)
* Use linear regression models
* Convert this problem into a classification problem
