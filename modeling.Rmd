---
title: "modeling"
author: "jvicentem"
date: "12 August 2017"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)

library(caret)

library(doMC)
registerDoMC(cores = 4)

library(parallel)

library(rattle)

library(rpart.plot)

library(dummies)
```

```{r cart}
# trControl <- trainControl(method = 'cv', number = 5)
# 
# fit.rpart1 <- train(y = train_set_1$quality,
#                     x = train_set_1 %>% select(-quality),
#                     method='rpart', 
#                     trControl=trControl, 
#                     metric='RMSE', 
#                     maximize=FALSE, 
#                     preProcess=c('center','scale'))
# 
# fit.rpart2 <- train(y = train_set_2$quality,
#                     x = train_set_2 %>% select(-quality),
#                     method='rpart', 
#                     trControl=trControl, 
#                     metric='RMSE', 
#                     maximize=FALSE, 
#                     preProcess=c('center','scale'))
# 
# fit.rpart3 <- train(y = train_set_3$quality,
#                     x = train_set_3 %>% select(-quality),
#                     method='rpart', 
#                     trControl=trControl, 
#                     metric='RMSE', 
#                     maximize=FALSE, 
#                     preProcess=c('center','scale'))
# 
# with(pmm_imp, train(form = quality ~ `fixed acidity` + `volatile acidity` + `citric acid` + `residual sugar` + `chlorides` + `free sulfur dioxide` + 
#                     `density` + `pH` + `sulphates` + `alcohol` + `wine_colour` + `other sulfur dioxide`,
#                     data = pmm_imp,
#                     method='rpart', 
#                     trControl=trControl, 
#                     metric='RMSE', 
#                     maximize=FALSE, 
#                     preProcess=c('center','scale')))

# Well, it seems I can't train 3 different dataset with 3 different 'other sulfur dioxide' imputed values and get a "pooled" tree by using pool function from mice library so I'm gonna stick with the set I used to perform PCA.

all_indexes <- 1:nrow(train_set)

mean_median_errors <- function(folds_errors) {
  square <- function(x) x^2
  
  mses <- foreach(x=folds_errors$RMSE, .combine='cbind') %do% square(x)
  
  print('MSE:')
  print(paste('Mean:', mean(mses)))
  print(paste('Median:', median(mses)))
  print('')
  print('RMSE:')
  print(paste('Mean:', mean(folds_errors$RMSE)))
  print(paste('Median:', median(folds_errors$RMSE)))
  print('')
  print('MAE:')
  print(paste('Mean:', mean(folds_errors$MAE)))
  print(paste('Median:', median(folds_errors$MAE)))  
}

rpart_with_folds <- function(train_df) {
  # Best cp: 0.005
  # tr <- train(y = train_df$quality,
  #       x = train_df %>% select(-quality),
  #       method='rpart',
  #       trControl=trainControl(method = 'cv', 
  #                    allowParallel = TRUE,
  #                    index=list(
  #                      f_1 = all_indexes[-folds_indexes$Fold1],
  #                      f_2 = all_indexes[-folds_indexes$Fold2],
  #                      f_3 = all_indexes[-folds_indexes$Fold3],
  #                      f_4 = all_indexes[-folds_indexes$Fold4],
  #                      f_5 = all_indexes[-folds_indexes$Fold5]
  #                    ),
  #                    indexOut=list(
  #                      f_1 = folds_indexes$Fold1,
  #                      f_2 = folds_indexes$Fold2,
  #                      f_3 = folds_indexes$Fold3,
  #                      f_4 = folds_indexes$Fold4,
  #                      f_5 = folds_indexes$Fold5
  #                    ),
  #                    verboseIter = TRUE,
  #                    savePredictions = TRUE
  #                  ),
  #       tuneGrid=expand.grid(cp=seq(0, 0.1, 0.005)),
  #       metric='RMSE',
  #       maximize=FALSE,
  #       preProcess=c('center','scale'))
  
  # Best depth: 8
  # tr <- train(y = train_df$quality,
  #       x = train_df %>% select(-quality),
  #       method='rpart2',
  #       trControl=trainControl(method = 'cv', 
  #                    allowParallel = TRUE,
  #                    index=list(
  #                      f_1 = all_indexes[-folds_indexes$Fold1],
  #                      f_2 = all_indexes[-folds_indexes$Fold2],
  #                      f_3 = all_indexes[-folds_indexes$Fold3],
  #                      f_4 = all_indexes[-folds_indexes$Fold4],
  #                      f_5 = all_indexes[-folds_indexes$Fold5]
  #                    ),
  #                    indexOut=list(
  #                      f_1 = folds_indexes$Fold1,
  #                      f_2 = folds_indexes$Fold2,
  #                      f_3 = folds_indexes$Fold3,
  #                      f_4 = folds_indexes$Fold4,
  #                      f_5 = folds_indexes$Fold5
  #                    ),
  #                    verboseIter = TRUE,
  #                    savePredictions = TRUE
  #                  ),
  #       tuneGrid=expand.grid(maxdepth=c(4,6,8,9,10,12,14,15,19,25)),
  #       metric='RMSE',
  #       maximize=FALSE,
  #       preProcess=c('center','scale'))
  
  # With max depth = 8, best cp is 0.005
  # tr <- train(y = train_df$quality,
  #       x = train_df %>% select(-quality),
  #       method='rpart',
  #       trControl=trainControl(method = 'cv', 
  #                    allowParallel = TRUE,
  #                    index=list(
  #                      f_1 = all_indexes[-folds_indexes$Fold1],
  #                      f_2 = all_indexes[-folds_indexes$Fold2],
  #                      f_3 = all_indexes[-folds_indexes$Fold3],
  #                      f_4 = all_indexes[-folds_indexes$Fold4],
  #                      f_5 = all_indexes[-folds_indexes$Fold5]
  #                    ),
  #                    indexOut=list(
  #                      f_1 = folds_indexes$Fold1,
  #                      f_2 = folds_indexes$Fold2,
  #                      f_3 = folds_indexes$Fold3,
  #                      f_4 = folds_indexes$Fold4,
  #                      f_5 = folds_indexes$Fold5
  #                    ),
  #                    verboseIter = TRUE,
  #                    savePredictions = TRUE
  #                  ),
  #       tuneGrid=expand.grid(cp=seq(0, 0.1, 0.005)),
  #       metric='RMSE',
  #       maximize=FALSE,
  #       control = rpart.control(maxdepth=8),
  #       preProcess=c('center','scale'))
  
  # Tweaking minsplit. By default it was 20. Best minsplit: ~100 
  #minbucket is by default minsplit/3  which I think is pretty fair.
  # tr <- train(y = train_df$quality,
  #       x = train_df %>% select(-quality),
  #       method='rpart',
  #       trControl=trainControl(method = 'cv', 
  #                    allowParallel = TRUE,
  #                    index=list(
  #                      f_1 = all_indexes[-folds_indexes$Fold1],
  #                      f_2 = all_indexes[-folds_indexes$Fold2],
  #                      f_3 = all_indexes[-folds_indexes$Fold3],
  #                      f_4 = all_indexes[-folds_indexes$Fold4],
  #                      f_5 = all_indexes[-folds_indexes$Fold5]
  #                    ),
  #                    indexOut=list(
  #                      f_1 = folds_indexes$Fold1,
  #                      f_2 = folds_indexes$Fold2,
  #                      f_3 = folds_indexes$Fold3,
  #                      f_4 = folds_indexes$Fold4,
  #                      f_5 = folds_indexes$Fold5
  #                    ),
  #                    verboseIter = TRUE,
  #                    savePredictions = TRUE
  #                  ),
  #       tuneGrid=expand.grid(cp=0.005),
  #       metric='RMSE',
  #       maximize=FALSE,
  #       control = rpart.control(maxdepth=8, minsplit=0.025*nrow(train_df)),
  #       preProcess=c('center','scale'))
  
  # Tweaking split criterion: information or gini (default). No difference between them
  tr <- train(y = train_df$quality,
              x = train_df %>% select(-quality),
              method='rpart',
              trControl=trainControl(method = 'cv', 
                          allowParallel = TRUE,
                          index=list(
                            f_1 = all_indexes[-folds_indexes$Fold1],
                            f_2 = all_indexes[-folds_indexes$Fold2],
                            f_3 = all_indexes[-folds_indexes$Fold3],
                            f_4 = all_indexes[-folds_indexes$Fold4],
                            f_5 = all_indexes[-folds_indexes$Fold5]
                          ),
                          indexOut=list(
                            f_1 = folds_indexes$Fold1,
                            f_2 = folds_indexes$Fold2,
                            f_3 = folds_indexes$Fold3,
                            f_4 = folds_indexes$Fold4,
                            f_5 = folds_indexes$Fold5
                          ),
                          verboseIter = TRUE,
                          savePredictions = TRUE
                        ),
              preProcess=c('center', 'scale'),
              tuneGrid=expand.grid(cp=0.005),
              metric='RMSE',
              maximize=FALSE,
              control = rpart.control(maxdepth=8, minsplit=0.025*nrow(train_df), parms=list(split='information')),
              preProcess=c('center','scale'))
  
  print(tr$resample)
  print('Best tune')
  print(tr$bestTune)
  
  return(tr)
}

error_metrics_cart <- cart_with_folds(train_set)$resample

mean_median_errors(error_metrics_cart)

# Now we train a cart with the whole train set using the best set of parameters tested in the different folds
cart <- train(y = train_set$quality,
              x = train_set %>% select(-quality),
              method='rpart',
              trControl=trainControl(method = 'none', allowParallel = TRUE),
              tuneGrid=expand.grid(cp=0.005),
              metric='RMSE',
              maximize=FALSE,
              control = rpart.control(maxdepth=8, minsplit=0.025*nrow(train_set)),
              preProcess=c('center','scale'))
  
fancyRpartPlot(cart$finalModel)
prp(cart$finalModel) 

plot(varImp(cart))
```

```{r rf}
rf_train_set <- train_set
names(rf_train_set) <- make.names(names(rf_train_set), unique=TRUE)
rf_dev_test <- dev_test
names(rf_dev_test) <- make.names(names(rf_dev_test), unique=TRUE)
  
rf_with_folds <- function(train_df) {
  # 8 is the best value for mtry
  # tr <- train(y=train_df$quality,
  #            x=train_df %>% select(-quality),
  #            method='ranger',
  #            trControl=trainControl(method = 'cv', 
  #                    allowParallel = TRUE,
  #                    index=list(
  #                      f_1 = all_indexes[-folds_indexes$Fold1],
  #                      f_2 = all_indexes[-folds_indexes$Fold2],
  #                      f_3 = all_indexes[-folds_indexes$Fold3],
  #                      f_4 = all_indexes[-folds_indexes$Fold4],
  #                      f_5 = all_indexes[-folds_indexes$Fold5]
  #                    ),
  #                    indexOut=list(
  #                      f_1 = folds_indexes$Fold1,
  #                      f_2 = folds_indexes$Fold2,
  #                      f_3 = folds_indexes$Fold3,
  #                      f_4 = folds_indexes$Fold4,
  #                      f_5 = folds_indexes$Fold5
  #                    ),
  #                    verboseIter = TRUE,
  #                    savePredictions = TRUE
  #                  ),
  #            preProc=c('center', 'scale'),
  #            tuneGrid=expand.grid(mtry = c(4,6,8,9,10), splitrule='variance'),
  #            num.trees=200,
  #            importance='impurity')
  
  # Trying to remove from the model some of the less important variables in
  # cart  
  tr <- train(y=train_df$quality,
             x=train_df %>% select(-quality, -pH, -other.sulfur.dioxide),
             method='ranger',
             trControl=trainControl(method = 'cv', allowParallel = TRUE,
               index=list(
                 f_1 = all_indexes[-folds_indexes$Fold1],
                 f_2 = all_indexes[-folds_indexes$Fold2],
                 f_3 = all_indexes[-folds_indexes$Fold3],
                 f_4 = all_indexes[-folds_indexes$Fold4],
                 f_5 = all_indexes[-folds_indexes$Fold5]
               ),
               indexOut=list(
                 f_1 = folds_indexes$Fold1,
                 f_2 = folds_indexes$Fold2,
                 f_3 = folds_indexes$Fold3,
                 f_4 = folds_indexes$Fold4,
                 f_5 = folds_indexes$Fold5
               ),                
               verboseIter = TRUE,
               savePredictions = TRUE
             ),
             preProc=c('center', 'scale'),
             tuneGrid=expand.grid(mtry = c(4,6,8,9), splitrule='variance'),
             num.trees=200,
             always.split.variables=c('volatile.acidity', 'alcohol'),
             importance='impurity')
  
  print(tr$resample)
  print('Best tune')
  print(tr$bestTune)
  
  return(tr)
}

error_metrics_rf <- rf_with_folds(rf_train_set)$resample

mean_median_errors(error_metrics_rf)

# Now we train a random forest model with the whole train set using the best set of parameters tested in the different folds
rf <- train(y = rf_train_set$quality,
            x = rf_train_set %>% select(-quality, -pH, -other.sulfur.dioxide),
            method='ranger',
            trControl=trainControl(method = 'none', allowParallel = TRUE),
            preProc=c('center', 'scale'),
            tuneGrid=expand.grid(mtry = 4, splitrule='variance'),
            num.trees=200,
            always.split.variables=c('volatile.acidity', 'alcohol'),
            importance='impurity')

plot(varImp(rf))
```


```{r lsvm}
svm_train_set <- train_set %>% mutate(wine_colour = as.factor(wine_colour))
svm_train_set <- cbind(svm_train_set %>% select(-wine_colour), dummy(svm_train_set$wine_colour, sep="."))
colnames(svm_train_set) <- c(colnames(svm_train_set)[1:12], c('wine_colour.red', 'wine_colour.white'))
svm_train_set <- svm_train_set %>% select(-wine_colour.red) # White wine is the majority level so that will be the base level in this dummy variable

svm_dev_test <- dev_test %>% mutate(wine_colour = as.factor(wine_colour))
svm_dev_test <- cbind(svm_dev_test %>% select(-wine_colour), dummy(svm_dev_test$wine_colour, sep="."))
colnames(svm_dev_test) <- c(colnames(svm_dev_test)[1:12], c('wine_colour.red', 'wine_colour.white'))
svm_dev_test <- svm_dev_test %>% select(-wine_colour.red) 

lsvm_with_folds <- function(train_df) {
  # tr <- train(y=train_df$quality,
  #             x=train_df %>% select (-quality),
  #             method='svmLinear2',
  #             trControl=trainControl(method = 'cv', allowParallel = TRUE,
  #              index=list(
  #                f_1 = all_indexes[-folds_indexes$Fold1],
  #                f_2 = all_indexes[-folds_indexes$Fold2],
  #                f_3 = all_indexes[-folds_indexes$Fold3],
  #                f_4 = all_indexes[-folds_indexes$Fold4],
  #                f_5 = all_indexes[-folds_indexes$Fold5]
  #              ),
  #              indexOut=list(
  #                f_1 = folds_indexes$Fold1,
  #                f_2 = folds_indexes$Fold2,
  #                f_3 = folds_indexes$Fold3,
  #                f_4 = folds_indexes$Fold4,
  #                f_5 = folds_indexes$Fold5
  #              ),
  #              verboseIter = TRUE,
  #              savePredictions = TRUE
  #             ),
  #             preProc=c('center', 'scale'),
  #             tuneGrid=expand.grid(cost=c(0, 0.01, 0.1, 0.2, 1))
  #          )
  
  tr <- train(y=train_df$quality,
              x=train_df %>% select (-quality, -pH),
              method='svmLinear2',
              trControl=trainControl(method = 'cv', allowParallel = TRUE,
               index=list(
                 f_1 = all_indexes[-folds_indexes$Fold1],
                 f_2 = all_indexes[-folds_indexes$Fold2],
                 f_3 = all_indexes[-folds_indexes$Fold3],
                 f_4 = all_indexes[-folds_indexes$Fold4],
                 f_5 = all_indexes[-folds_indexes$Fold5]
               ),
               indexOut=list(
                 f_1 = folds_indexes$Fold1,
                 f_2 = folds_indexes$Fold2,
                 f_3 = folds_indexes$Fold3,
                 f_4 = folds_indexes$Fold4,
                 f_5 = folds_indexes$Fold5
               ),
               verboseIter = TRUE,
               savePredictions = TRUE
              ),
              preProc=c('center', 'scale'),
              tuneGrid=expand.grid(cost=c(0, 0.01, 0.015, 0.02, 0.1, 0.2, 1))
           )  
  
  print(tr$resample)
  print('Best tune')
  print(tr$bestTune)
  
  return(tr)
}

lsvm_train <- lsvm_with_folds(svm_train_set)

error_metrics_lsvm <- lsvm_train$resample
mean_median_errors(error_metrics_lsvm)

lsvm <- train(y=svm_train_set$quality,
              x=svm_train_set %>% select (-quality, -pH),
              method='svmLinear2',
              trControl=trainControl(method = 'none', allowParallel = TRUE),
              preProc=c('center', 'scale'),
              tuneGrid=expand.grid(cost=c(0.01))
             )

varImp(lsvm)
plot(varImp(lsvm))
```

```{r psvm}
# psvm_with_folds <- function(train_df) {
#   tr <- train(y = train_df$quality,
#               x = train_df %>% select(-quality),
#               method='svmPoly',
#               trControl=trainControl(method = 'cv', allowParallel = TRUE,
#                 index=list(
#                   f_1 = all_indexes[-folds_indexes$Fold1],
#                   f_2 = all_indexes[-folds_indexes$Fold2],
#                   f_3 = all_indexes[-folds_indexes$Fold3],
#                   f_4 = all_indexes[-folds_indexes$Fold4],
#                   f_5 = all_indexes[-folds_indexes$Fold5]
#                 ),
#                 indexOut=list(
#                   f_1 = folds_indexes$Fold1,
#                   f_2 = folds_indexes$Fold2,
#                   f_3 = folds_indexes$Fold3,
#                   f_4 = folds_indexes$Fold4,
#                   f_5 = folds_indexes$Fold5
#                 ),
#                 verboseIter = TRUE,
#                 savePredictions = TRUE
#               ),
#               preProcess=c('center', 'scale'),
#               tuneGrid=expand.grid(C=c(0, 0.01, 0.1, 0.015, 0.02, 1), degree=c(2, 4, 6), scale=c(1))
#         )  
#   
#   print(tr$resample)
#   print('Best tune')
#   print(tr$bestTune)
#   
#   return(tr)
# }
# 
# psvm_train <- psvm_with_folds(svm_train_set[1:5,])
# error_metrics_psvm <- psvm_train$resample
# 
# mean_median_errors(error_metrics_psvm)
# 
# psvm <- train(y=svm_train_set$quality,
#               x=svm_train_set %>% select (-quality, -pH),
#               method='svmPoly',
#               trControl=trainControl(method = 'none', allowParallel = TRUE),
#               preProc=c('center', 'scale'),
#               tuneGrid=expand.grid(C=c(0.01), degree=c(2), scale=c(1))
#              )
# 
# varImp(psvm)
# plot(varImp(psvm))
```

```{r xgboost}
xgb_with_folds <- function(train_df) {
  tr <- train(y=train_df$quality,
             x=train_df %>% select(-quality, -wine_colour.white),
             method='xgbLinear',
             trControl=trainControl(method = 'cv', allowParallel = TRUE,
               index=list(
                 f_1 = all_indexes[-folds_indexes$Fold1],
                 f_2 = all_indexes[-folds_indexes$Fold2],
                 f_3 = all_indexes[-folds_indexes$Fold3],
                 f_4 = all_indexes[-folds_indexes$Fold4],
                 f_5 = all_indexes[-folds_indexes$Fold5]
               ),
               indexOut=list(
                 f_1 = folds_indexes$Fold1,
                 f_2 = folds_indexes$Fold2,
                 f_3 = folds_indexes$Fold3,
                 f_4 = folds_indexes$Fold4,
                 f_5 = folds_indexes$Fold5
               ),                
               verboseIter = TRUE,
               savePredictions = TRUE
             ),
             preProc=c('center', 'scale'),
             tuneGrid=expand.grid(nrounds = c(1000), lambda = c(0, 0.5, 1), alpha = c(0, 0.5, 0.8, 1), eta = c(0.005, 0.01, 0.02))
             )
  
  print(tr$resample)
  print('Best tune')
  print(tr$bestTune)
  
  return(tr)
}

xgb_train <- xgb_with_folds(svm_train_set)
error_metrics_xgb <- xgb_train$resample

mean_median_errors(error_metrics_xgb)

xgb <- train(y=svm_train_set$quality,
              x=svm_train_set %>% select (-quality, -wine_colour.white),
              method='xgbLinear',
              trControl=trainControl(method = 'none', allowParallel = TRUE),
              preProc=c('center', 'scale'),
              # Though CV output says eta = 0.005 is better, I'm gonna stick with 0.01 because
              #the error metrics are equal and this last value can make training a little bit faster
              tuneGrid=expand.grid(nrounds = c(10000), lambda = c(0), alpha = c(1), eta = c(0.01))
             )

varImp(xgb)
plot(varImp(xgb))
```

I've removed the less important variables from almost every model to make them more parsimonious and improve the training times having in mind the penalization in prediction this takes. 
